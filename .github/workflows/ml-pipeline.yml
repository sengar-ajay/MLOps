name: ML Model Training & Validation

# Add permissions for GitHub Actions to write to repository
permissions:
  contents: write
  actions: read

on:
  schedule:
    # Run weekly on Sundays at 02:00 UTC for model retraining
    - cron: "0 2 * * 0"
  workflow_dispatch:
    inputs:
      retrain_model:
        description: "Force model retraining"
        required: false
        default: false
        type: boolean
      data_validation_only:
        description: "Run data validation only"
        required: false
        default: false
        type: boolean
  push:
    paths:
      - "src/model_training.py"
      - "src/data_preprocessing.py"
      - "data/**"
      - "triggers/*.trigger"

jobs:
  data-validation:
    runs-on: ubuntu-latest
    outputs:
      data-changed: ${{ steps.check-data.outputs.changed }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Check data changes
        id: check-data
        run: |
          if git diff --name-only HEAD~1 HEAD | grep -q "^data/"; then
            echo "changed=true" >> $GITHUB_OUTPUT
          else
            echo "changed=false" >> $GITHUB_OUTPUT
          fi

      - name: Validate data quality
        run: |
          python -c "
          import pandas as pd
          import numpy as np
          import os

          def validate_data_file(filepath, expected_shape_min=None):
              if not os.path.exists(filepath):
                  print(f'WARNING: {filepath} not found - may be generated during pipeline')
                  return 'missing'
                  
              df = pd.read_csv(filepath)
              print(f'{filepath}: Shape {df.shape}')
              
              # Check for missing values
              missing = df.isnull().sum().sum()
              if missing > 0:
                  print(f'WARNING: {filepath}: {missing} missing values found')
              
              # Check minimum shape if provided
              if expected_shape_min and df.shape[0] < expected_shape_min[0]:
                  print(f'ERROR: {filepath}: Insufficient data rows {df.shape[0]} < {expected_shape_min[0]}')
                  return False
                  
              return True

          # Validate training data
          files_to_check = [
              ('data/X_train.csv', (100, 8)),
              ('data/X_test.csv', (50, 8)),
              ('data/y_train.csv', (100, 1)),
              ('data/y_test.csv', (50, 1))
          ]

          all_valid = True
          missing_files = 0
          for filepath, min_shape in files_to_check:
              result = validate_data_file(filepath, min_shape)
              if result == 'missing':
                  missing_files += 1
              elif result == False:
                  all_valid = False

          # Only fail if files exist but are invalid, not if they're missing
          if not all_valid:
              print('ERROR: Data validation failed - existing files have issues')
              exit(1)
          elif missing_files > 0:
              print(f'INFO: {missing_files} data files missing but will be generated during pipeline')
              print('Data validation passed - proceeding with pipeline')
          else:
              print('All data validation checks passed')
          "

  model-training:
    runs-on: ubuntu-latest
    needs: data-validation
    if: needs.data-validation.outputs.data-changed == 'true' || github.event.inputs.retrain_model == 'true' || github.event_name == 'schedule'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run data preprocessing
        run: |
          python src/data_preprocessing.py

      - name: Train model
        run: |
          python src/model_training.py

      - name: Validate trained model
        run: |
          python -c "
          import joblib
          import json
          import os
          import numpy as np

          # Load model metrics
          if os.path.exists('models/best_model_metrics.json'):
              with open('models/best_model_metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              print('Model Metrics:')
              for key, value in metrics.items():
                  print(f'  {key}: {value:.4f}')
              
              # Validate model performance thresholds
              if 'test_r2_score' in metrics:
                  if metrics['test_r2_score'] < 0.6:
                      print('ERROR: Model R¬≤ score below threshold (0.6)')
                      exit(1)
                  else:
                      print('Model R¬≤ score meets threshold')
              
              if 'test_rmse' in metrics:
                  if metrics['test_rmse'] > 1.0:
                      print('ERROR: Model RMSE above threshold (1.0)')
                      exit(1)
                  else:
                      print('Model RMSE meets threshold')
          else:
              print('ERROR: Model metrics file not found')
              exit(1)

          # Test model loading and prediction
          try:
              model = joblib.load('models/best_model.pkl')
              scaler = joblib.load('data/scaler.pkl')
              
              # Test prediction with sample data
              test_input = np.array([[8.3252, 41.0, 6.98, 1.02, 322.0, 2.56, 37.88, -122.23]])
              test_input_scaled = scaler.transform(test_input)
              prediction = model.predict(test_input_scaled)
              
              print(f'Model prediction test successful: {prediction[0]:.4f}')
              
          except Exception as e:
              print(f'ERROR: Model loading/prediction failed: {e}')
              exit(1)
          "

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trained-model-${{ github.run_number }}
          path: |
            models/best_model.pkl
            models/linear_regression_model.pkl
            models/random_forest_model.pkl
            models/gradient_boosting_model.pkl
            data/scaler.pkl
            models/best_model_metrics.json
            models/all_models_comparison.json

      - name: Commit updated model (if on main branch)
        if: github.ref == 'refs/heads/main'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          if [ -f "models/best_model.pkl" ] && [ -f "data/scaler.pkl" ]; then
            git add models/best_model.pkl models/*_model.pkl data/scaler.pkl models/best_model_metrics.json models/all_models_comparison.json
            
            if git diff --cached --quiet; then
              echo "No model changes to commit"
            else
              git commit -m "Auto-update trained model [skip ci]"
              git push
              echo "Updated model committed to repository"
            fi
          fi

  model-comparison:
    runs-on: ubuntu-latest
    needs: [data-validation, model-training]
    if: needs.model-training.result == 'success'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download new model artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-model-${{ github.run_number }}
          path: ./new-model/

      - name: Compare model performance
        run: |
          python -c "
          import json
          import os

          print('=' * 80)
          print('COMPREHENSIVE MODEL TRAINING & COMPARISON REPORT')
          print('=' * 80)

          # Load new comprehensive model comparison
          if os.path.exists('new-model/models/all_models_comparison.json'):
              with open('new-model/models/all_models_comparison.json', 'r') as f:
                  all_models = json.load(f)
              
              print('\nüìä INDIVIDUAL MODEL PERFORMANCE:')
              print('-' * 50)
              
              # Sort models by RMSE (best to worst)
              sorted_models = sorted(all_models, key=lambda x: x['rmse'])
              
              for i, model in enumerate(sorted_models):
                  rank = 'ü•á' if i == 0 else 'ü•à' if i == 1 else 'ü•â' if i == 2 else f'{i+1}.'
                  print(f'\n{rank} {model[\"model_name\"]} ({model[\"model_type\"]}):')
                  print(f'   RMSE: {model[\"rmse"]:.6f}')
                  print(f'   MAE:  {model[\"mae\"]:.6f}')
                  print(f'   R¬≤:   {model[\"r2\"]:.6f}')
              
              print(f'\nüèÜ BEST MODEL: {sorted_models[0][\"model_name\"]}')
              print(f'   Performance: RMSE={sorted_models[0][\"rmse\"]:.6f}, R¬≤={sorted_models[0][\"r2\"]:.6f}')

          # Load current vs new comparison
          current_metrics = {}
          if os.path.exists('models/best_model_metrics.json'):
              with open('models/best_model_metrics.json', 'r') as f:
                  current_metrics = json.load(f)

          new_metrics = {}
          if os.path.exists('new-model/models/best_model_metrics.json'):
              with open('new-model/models/best_model_metrics.json', 'r') as f:
                  new_metrics = json.load(f)

          if current_metrics and new_metrics:
              print(f'\nüìà BEST MODEL COMPARISON (Current vs New):')
              print('-' * 50)
              
              metrics_to_compare = ['rmse', 'mae', 'r2']
              
              for metric in metrics_to_compare:
                  if metric in current_metrics and metric in new_metrics:
                      current_val = current_metrics[metric]
                      new_val = new_metrics[metric]
                      
                      if metric == 'r2':  # Higher is better
                          improvement = new_val - current_val
                          status = 'üìà IMPROVED' if improvement > 0 else 'üìâ DEGRADED' if improvement < 0 else '‚û°Ô∏è UNCHANGED'
                      else:  # Lower is better (RMSE, MAE)
                          improvement = current_val - new_val
                          status = 'üìà IMPROVED' if improvement > 0 else 'üìâ DEGRADED' if improvement < 0 else '‚û°Ô∏è UNCHANGED'
                      
                      print(f'{metric.upper()}:')
                      print(f'  Current: {current_val:.6f}')
                      print(f'  New:     {new_val:.6f}')
                      print(f'  Status:  {status} ({improvement:+.6f})')
                      print()

          print('=' * 80)
          "

      - name: Generate model report
        run: |
          echo '# Comprehensive Model Training Report' > model-report.md
          echo '' >> model-report.md
          echo '## Training Summary' >> model-report.md
          echo '- **Trigger**: ${{ github.event_name }}' >> model-report.md
          echo '- **Timestamp**: '$(date -u) >> model-report.md
          echo '- **Commit**: ${{ github.sha }}' >> model-report.md
          echo '' >> model-report.md

          # Individual Models Performance
          if [ -f "new-model/models/all_models_comparison.json" ]; then
            echo '## Individual Model Performance' >> model-report.md
            echo '' >> model-report.md
            python -c "
          import json
          with open('new-model/models/all_models_comparison.json', 'r') as f:
              all_models = json.load(f)

          # Sort by RMSE (best first)
          sorted_models = sorted(all_models, key=lambda x: x['rmse'])

          for i, model in enumerate(sorted_models):
              rank = 'ü•á ' if i == 0 else 'ü•à ' if i == 1 else 'ü•â ' if i == 2 else f'{i+1}. '
              print(f'### {rank}{model[\"model_name\"]} ({model[\"model_type\"]})')
              print(f'- **RMSE**: {model[\"rmse\"]:.6f}')
              print(f'- **MAE**: {model[\"mae\"]:.6f}')
              print(f'- **R¬≤ Score**: {model[\"r2\"]:.6f}')
              print('')
          " >> model-report.md
          fi

          # Best Model Details
          echo '## Best Model Details' >> model-report.md
          echo '' >> model-report.md
          if [ -f "new-model/models/best_model_metrics.json" ]; then
            python -c "
          import json
          with open('new-model/models/best_model_metrics.json', 'r') as f:
              metrics = json.load(f)

          print(f'**Selected Model**: {metrics.get(\"best_model_name\", \"Unknown\")} ({metrics.get(\"best_model_type\", \"Unknown\")})')
          print('')
          print('**Performance Metrics**:')
          for key, value in metrics.items():
              if key in ['rmse', 'mae', 'r2']:
                  print(f'- **{key.upper()}**: {value:.6f}')
          print('')
          if 'training_timestamp' in metrics:
              print(f'**Training Completed**: {metrics[\"training_timestamp\"]}')
          " >> model-report.md
          fi

          echo '' >> model-report.md
          echo '## Model Artifacts Generated' >> model-report.md
          echo '- Individual model files for all algorithms' >> model-report.md
          echo '- Comprehensive comparison data (JSON)' >> model-report.md
          echo '- Best model selection and preprocessing scaler' >> model-report.md
          echo '- Detailed performance metrics for analysis' >> model-report.md

      - name: Upload model report
        uses: actions/upload-artifact@v4
        with:
          name: model-training-report-${{ github.run_number }}
          path: model-report.md

  notify-completion:
    runs-on: ubuntu-latest
    needs: [data-validation, model-training, model-comparison]
    if: always()

    steps:
      - name: Training completion notification
        run: |
          if [ "${{ needs.model-training.result }}" == "success" ]; then
            echo "Model training completed successfully!"
            echo "Data validation: ${{ needs.data-validation.result }}"
            echo "Model training: ${{ needs.model-training.result }}"
            echo "Model comparison: ${{ needs.model-comparison.result }}"
          else
            echo "Model training workflow failed"
            echo "Data validation: ${{ needs.data-validation.result }}"
            echo "Model training: ${{ needs.model-training.result }}"
            echo "Model comparison: ${{ needs.model-comparison.result }}"
          fi
