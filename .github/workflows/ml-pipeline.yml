name: ML Model Training & Validation

on:
  schedule:
    # Run weekly on Sundays at 02:00 UTC for model retraining
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      retrain_model:
        description: 'Force model retraining'
        required: false
        default: 'false'
        type: boolean
      data_validation_only:
        description: 'Run data validation only'
        required: false
        default: 'false'
        type: boolean
  push:
    paths:
      - 'src/model_training.py'
      - 'src/data_preprocessing.py'
      - 'data/**'

jobs:
  data-validation:
    runs-on: ubuntu-latest
    outputs:
      data-changed: ${{ steps.check-data.outputs.changed }}
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Check data changes
      id: check-data
      run: |
        if git diff --name-only HEAD~1 HEAD | grep -q "^data/"; then
          echo "changed=true" >> $GITHUB_OUTPUT
        else
          echo "changed=false" >> $GITHUB_OUTPUT
        fi

    - name: Validate data quality
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        import os
        
        def validate_data_file(filepath, expected_shape_min=None):
            if not os.path.exists(filepath):
                print(f'âŒ {filepath} not found')
                return False
                
            df = pd.read_csv(filepath)
            print(f'âœ… {filepath}: Shape {df.shape}')
            
            # Check for missing values
            missing = df.isnull().sum().sum()
            if missing > 0:
                print(f'âš ï¸  {filepath}: {missing} missing values found')
            
            # Check minimum shape if provided
            if expected_shape_min and df.shape[0] < expected_shape_min[0]:
                print(f'âŒ {filepath}: Insufficient data rows {df.shape[0]} < {expected_shape_min[0]}')
                return False
                
            return True
        
        # Validate training data
        files_to_check = [
            ('data/X_train.csv', (100, 8)),
            ('data/X_test.csv', (50, 8)),
            ('data/y_train.csv', (100, 1)),
            ('data/y_test.csv', (50, 1))
        ]
        
        all_valid = True
        for filepath, min_shape in files_to_check:
            if not validate_data_file(filepath, min_shape):
                all_valid = False
        
        if not all_valid:
            print('âŒ Data validation failed')
            exit(1)
        else:
            print('âœ… All data validation checks passed')
        "

  model-training:
    runs-on: ubuntu-latest
    needs: data-validation
    if: needs.data-validation.outputs.data-changed == 'true' || github.event.inputs.retrain_model == 'true' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run data preprocessing
      run: |
        python src/data_preprocessing.py

    - name: Train model
      run: |
        python src/model_training.py

    - name: Validate trained model
      run: |
        python -c "
        import joblib
        import json
        import os
        import numpy as np
        
        # Load model metrics
        if os.path.exists('best_model_metrics.json'):
            with open('best_model_metrics.json', 'r') as f:
                metrics = json.load(f)
            
            print('ğŸ“Š Model Metrics:')
            for key, value in metrics.items():
                print(f'  {key}: {value:.4f}')
            
            # Validate model performance thresholds
            if 'test_r2_score' in metrics:
                if metrics['test_r2_score'] < 0.6:
                    print('âŒ Model RÂ² score below threshold (0.6)')
                    exit(1)
                else:
                    print('âœ… Model RÂ² score meets threshold')
            
            if 'test_rmse' in metrics:
                if metrics['test_rmse'] > 1.0:
                    print('âŒ Model RMSE above threshold (1.0)')
                    exit(1)
                else:
                    print('âœ… Model RMSE meets threshold')
        else:
            print('âŒ Model metrics file not found')
            exit(1)
        
        # Test model loading and prediction
        try:
            model = joblib.load('models/best_model.pkl')
            scaler = joblib.load('data/scaler.pkl')
            
            # Test prediction with sample data
            test_input = np.array([[8.3252, 41.0, 6.98, 1.02, 322.0, 2.56, 37.88, -122.23]])
            test_input_scaled = scaler.transform(test_input)
            prediction = model.predict(test_input_scaled)
            
            print(f'âœ… Model prediction test successful: {prediction[0]:.4f}')
            
        except Exception as e:
            print(f'âŒ Model loading/prediction failed: {e}')
            exit(1)
        "

    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: trained-model-${{ github.run_number }}
        path: |
          models/best_model.pkl
          data/scaler.pkl
          best_model_metrics.json

    - name: Commit updated model (if on main branch)
      if: github.ref == 'refs/heads/main'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if [ -f "models/best_model.pkl" ] && [ -f "data/scaler.pkl" ]; then
          git add models/best_model.pkl data/scaler.pkl best_model_metrics.json
          
          if git diff --cached --quiet; then
            echo "No model changes to commit"
          else
            git commit -m "ğŸ¤– Auto-update trained model [skip ci]"
            git push
            echo "âœ… Updated model committed to repository"
          fi
        fi

  model-comparison:
    runs-on: ubuntu-latest
    needs: [data-validation, model-training]
    if: needs.model-training.result == 'success'
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download new model artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-model-${{ github.run_number }}
        path: ./new-model/

    - name: Compare model performance
      run: |
        python -c "
        import json
        import os
        
        # Load current model metrics
        current_metrics = {}
        if os.path.exists('best_model_metrics.json'):
            with open('best_model_metrics.json', 'r') as f:
                current_metrics = json.load(f)
        
        # Load new model metrics
        new_metrics = {}
        if os.path.exists('new-model/best_model_metrics.json'):
            with open('new-model/best_model_metrics.json', 'r') as f:
                new_metrics = json.load(f)
        
        print('ğŸ“Š Model Performance Comparison:')
        print('=' * 50)
        
        metrics_to_compare = ['test_r2_score', 'test_rmse', 'test_mae']
        
        for metric in metrics_to_compare:
            if metric in current_metrics and metric in new_metrics:
                current_val = current_metrics[metric]
                new_val = new_metrics[metric]
                
                if metric == 'test_r2_score':  # Higher is better
                    improvement = new_val - current_val
                    status = 'ğŸ“ˆ' if improvement > 0 else 'ğŸ“‰' if improvement < 0 else 'â¡ï¸'
                else:  # Lower is better (RMSE, MAE)
                    improvement = current_val - new_val
                    status = 'ğŸ“ˆ' if improvement > 0 else 'ğŸ“‰' if improvement < 0 else 'â¡ï¸'
                
                print(f'{metric}:')
                print(f'  Current: {current_val:.4f}')
                print(f'  New:     {new_val:.4f}')
                print(f'  Change:  {status} {improvement:+.4f}')
                print()
        "

    - name: Generate model report
      run: |
        echo '# ğŸ¤– Model Training Report' > model-report.md
        echo '' >> model-report.md
        echo '## Training Summary' >> model-report.md
        echo '- **Trigger**: ${{ github.event_name }}' >> model-report.md
        echo '- **Timestamp**: '$(date -u) >> model-report.md
        echo '- **Commit**: ${{ github.sha }}' >> model-report.md
        echo '' >> model-report.md
        echo '## Model Metrics' >> model-report.md
        
        if [ -f "new-model/best_model_metrics.json" ]; then
          python -c "
          import json
          with open('new-model/best_model_metrics.json', 'r') as f:
              metrics = json.load(f)
          
          for key, value in metrics.items():
              print(f'- **{key}**: {value:.4f}')
          " >> model-report.md
        fi

    - name: Upload model report
      uses: actions/upload-artifact@v3
      with:
        name: model-training-report-${{ github.run_number }}
        path: model-report.md

  notify-completion:
    runs-on: ubuntu-latest
    needs: [data-validation, model-training, model-comparison]
    if: always()
    
    steps:
    - name: Training completion notification
      run: |
        if [ "${{ needs.model-training.result }}" == "success" ]; then
          echo "ğŸ‰ Model training completed successfully!"
          echo "âœ… Data validation: ${{ needs.data-validation.result }}"
          echo "âœ… Model training: ${{ needs.model-training.result }}"
          echo "âœ… Model comparison: ${{ needs.model-comparison.result }}"
        else
          echo "âŒ Model training workflow failed"
          echo "ğŸ“Š Data validation: ${{ needs.data-validation.result }}"
          echo "ğŸ¤– Model training: ${{ needs.model-training.result }}"
          echo "ğŸ“ˆ Model comparison: ${{ needs.model-comparison.result }}"
        fi